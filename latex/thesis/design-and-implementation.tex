%
% $Id: chapter3.tex 3915 2009-06-18 13:28:32Z sliske $
%
%%\pagestyle{scrheadings}
%%\ohead[]{}
%%\ihead[]{}
%%\chead[]{}
%%\ofoot[\pagemark]{\pagemark}
%%\ifoot[]{}
\chapter{Design \& Implementation}
\label{sec:design}

The following section introduces the design and implementation of the proposed \ac{IPC} architecture and the proof of concept \ac{IPS}, as well as auxiliary libraries and applications.

\section{Requirements}

The primary goal of the new \ac{IPC} based logging architecture, is the ability to handle high load scenarios, like the \ac{DoS} scenario discussed in section \ref{sec:fail2ban}.
This includes the basic functional requirement, of offering an API, that can be used to send data to and receive data from different processes. Beyond that,
the architecture should be usable in a realistic context. For this purpose, the following requirements should be satisfied, in order of importance.
\begin{itemize}
    \item \textbf{Low latency} \\
    As discussed in \ref{sec:fail2ban},  the primary issue of file based logging, in the context of \ac{IPS}, appears to be a lack of transmission speed. Hence,
    offering low latency data transfer, is a critical aspect of the new architecture. In a \ac{DoS} scenario, the \ac{IPS} need to receive information
    about malicious clients as fast as possible, in order to take countermeasures, that lower the load on the system. 
    \item \textbf{Low overhead} \\
    Another problem with file based logging is, that both read and write operations require systemcalls, which lead to costly context switches between application and kernel. Using traditional
    file I/O, read and write operations are blocking, meaning that a program can only resume, once the operation has concluded. In a high load scenario, where potentially
    millions of log messages are written per second, this can cause significant communication overhead for both reader and writer. It also looses critical execution time to waiting.
   On the side of the writing server, this lowers the availability of the service, while for the reading \ac{IPS}, responsiveness to the ongoing attack is reduced. The new architecture should therefore offer low overhead communication,
    ensuring that both writer and reader are not spending a majority of their execution time writing or receiving log messages.
    \item \textbf{High bandwidth} \\
    In conjunction with  low latency, high bandwidth is required, to ensure a fast transfer of large amounts of data. When processing millions of log events per second, the architecture
    needs to be able to execute a single data transfer quickly, but also allow for a large amount of data to be transmitted at once, to avoid a bottleneck between writer and reader.
    \item \textbf{Reliability} \\
    While not the primary concern, the architecture should, in principal, be able to transfer log messages reliably, even under a high load. Hence, message loss or corruption 
    should be kept to a minimum. This is to ensure, that the \ac{IPS} or other reading application are aware of the entirety of all logged events and no potentially security relevant event is missed. 
    \item \textbf{Scalability} \\
    An advantage of file based logging is, that logged events are stored persistently and also accessible to a multitude of applications. Log data generated by an application, may be relevant in different
    contexts, such as intrusion prevention, \ac{SIEM} or other, non-security related uses. Therefore, the architecture should be able to scale to multiple readers, that can access the log data in parallel. 
    \item \textbf{Integrability / Portability} \\
    Finally, to be of use in a realistic context, it is required that the architecture is able to integrate with existing applications. Since the goal if this thesis is a proof of concept and not an 
    API ready for use in production, this will be a lower priority requirement. However, the design should at a minimum offer a well defined and easily usable API, that can be realistically integrated into a real application
    and potentially further developed beyond the scope of this thesis. As part of that, the design should be portable to different systems and ideally not rely on special hardware or software support, other than what
    is commonly present on most Linux systems. The architecture should also be flexible i.e. impose as little restrictions as possible on the user, in order to be adaptable to a wide range of use cases.  

\end{itemize}

The architecture proposed on this thesis will likely not be able to satisfy all listed requirements to the fullest. Potential tradeoffs between requirements exists and will have to be considered in the following
design. Where tradeoffs are impossible to avoid, the architecture should ideally offer configuration options, that allow the user to decide, which requirements should be prioritized.       

\section{Abstract Architecture}

To further illustrate the purpose and requirements of the desired architecture, an abstract architecture was developed.
The goal is to provide a high-level design, that formalizes a set of functional requirement, but leaves enough
flexibility to be implemented with different \ac{IPC} solutions.  

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{images/meta_ipc_architecture.png}
    \caption[IPC Architecture]{Abstract \ac{IPC} architecture}
    \label{fig:abstract_architecture}
\end{figure}

Figure \ref{fig:abstract_architecture} presents the abstract design for the proposed \ac{IPC} architecture.
The assumed use case is a producer consumer scenario, in which a single application writes log messages, that 
are read by $k$ receiving processes. The transfer of data is assumed to be unidirectional, i.e. the roles of reader and writer are static for the duration of the
communication. An exception to this can be auxiliary messages, for the setup, teardown or reconfiguration of the communication, that may require readers to send
data to the writer. The writer is assumed to be a server, that receives client request over a network.
On modern multi-core system, servers usually use multiple threads to handle incoming request.
This allows for the parallel distribution of work onto the available CPU cores, making more efficient use of the systems resources. To efficiently accommodate a 
multi-threaded writer, the \ac{IPC} architecture is required to support a thread-safe write operation. Hence, it must be possible to
write in parallel from $n$ threads, without race conditions, that could cause data corruption, or the need for additional thread synchronization    
on the writer side. On the reader side, the architecture must be able to support multiple reading processes, that each receive the entirety
of the written messages and are able to read in parallel. To allow load balancing on the reader side as well, the read operation
should also be capable of multi-threading. For the multi-threaded read within a reading process, the operation should enable a balanced and starvation-free distribution 
of messages to the calling threads.  
\par
Given the requirements and abstract architecture, the goal is to find a concrete implementation, that satisfied the proposed design.


\section{Choice of IPC Type}

During the development of this thesis, all of the \ac{IPC} mechanism introduced in \ref{sec:ipc_types} where considered, as suitable candidates, for the proposed architecture. The initial
objective was, to create multiple implementations, that each use a different \ac{IPC} type. This would have allowed to compare their performance empirically and make a more informed decision, as to 
which \ac{IPC} type is most appropriate for the given requirements.
Due to time issues, the idea was abandoned, in favor of a single implementation.
\par
Shared memory was the first candidate considered for the design. The main reason for this is speed, since low latency communication is 
the primary requirement for the proposed architecture. As aforementioned, shared memory has the significant advantage of not involving the 
kernel for read or write operations. As long as the overhead for synchronizing the communicating processes is kept low, shared memory essentially
operates at the speed of normal memory accesses, which is unlikely to be surpassed by any kernel-based \ac{IPC} method. Shared memory grants 
great flexibility in the design of the API, which also provides a challenge. To the best of my knowledge, no standard for the shared memory based
transmission of log messages exists, making it harder to integrate the architecture with real world applications. Also, synchronization for multi-threaded
and multi-process use, will have to be implemented as part of the API. Another downside to shared memory is, that
it does not trivially scale beyond the local system, which could be desired for more complex systems, where \ac{IPS} and server do not share the same host.
\par 
Named pipes are another potential option to implement the design. Unlike shared memory, they already provide a well defined read and write API,
that can be integrated into the proof of concept \ac{IPS}, as well as a log aggregator such as Rsyslog or Logstash. Additionally, pipes are safe for multi-process
and multi-threaded reading. Multi-threaded writing, by a single process, is also thread-safe, as long as the data written is smaller than \texttt{PIPE\_BUF} (4096 bytes on Linux)\cite{pipe}.
A downside to the usage of pipes, are the required systemcalls, for \ac{IO} operations. Especially, since thread-safe writing only allows for small message sizes, making it more complicated 
to reduce the amount of systemcalls, through batched write operations. 
\par  
A socket based implementation was also considered. A straightforward strategy for implementing the one-to-many communication, required by the abstract architecture, would be the use of \ac{IPv6}
multicast. \ac{IPv6} multicast allows several processes to join a distinct multi-cast group, where each member of the group receives the packets sent to the multicast address. This would additionally
allow to support the syslog protocol, which could make for easier integration of the architecture with existing logging applications. The architecture could also scale beyond the local system to support 
network-based logging. A downside of a socket based approach, same as for named pipes, is the overhead associated with involving the kernel.
\par 
Lastly, a message queue design, using the ZeroMQ library was considered. ZeroMQ is a logical choice, since the library natively supports a publish-subscribe pattern, that could be used to
implement the required one-to-many communication. In addition to that, Rsyslog and Logstash support ZeroMQ sockets as input, which would allow easy integration with the architecture.
One issue with a ZeroMQ based implementation is, that the \texttt{SO\_REUSEPORT} flag, for reusing a single socket among several threads, is not supported by ZeroMQ \cite{zeromq_issue}. 
ZeroMQ sockets are not otherwise thread-safe\footnote{\url{https://zguide.zeromq.org/docs/chapter2/}} \cite{zeromq}. Hence, to the best of my knowledge, a publish-subscribe pattern with a 
multi-threaded producer can not be trivially implemented, without external synchronization. Using external synchronization would defeat the purpose of multi-threaded
writing, since only one thread could access the socket at a time. 
\par 
Ultimately, the decision was made, to develope a shared memory based implementation of the architecture. The main motivation for this is, that it is the most promising
candidate for low-latency, high bandwidth communication, which is a critical aspect of the design. However, the proof of concept \ac{IPS} will be designed as modular as possible, so
that implementations based on other \ac{IPC} types can be integrated in potential future development. 

\section{Shared Memory API}

The following section presents the design and implementation of the new shared memory based architecture, for the transmission
of log messages. 

\subsection{Design} \label{sec:rbuf_design}
For the base structure of the shared memory design, a ring buffer was chosen. A ring buffer is a queue-like data structure, used to store
multiple entries in a sequence. The entries are processed in \ac{FIFO} manner, hence data is being read in the same order in that is was written. 
Ring buffers are a commonly used structure to buffer data streams, especially for network applications \cite{barrington2015}.
They are usually realized as a fixed size array with additional pointers, that indicate the current location
of readers and writers within the buffer. When the end of the buffer is reached, during a read or write operation, the pointer
wraps around to the beginning of the buffer, treating the array, as if beginning and end where connected in a circular shape. 

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{images/shm_architecture.png}
    \caption[Shared Memory Architecture]{Architecture for a single-writer, multi-reader shared memory ring buffer for the transmission
    of log messages.}
    \label{fig:shm_architecture}
\end{figure}

Figure \ref{fig:shm_architecture} display the proposed shared memory architecture, adapted from the abstract design in Figure \ref{fig:abstract_architecture}.
The shared memory region consists of a global header, as well as a variable amount of segments,
that each have their own segments header. Each segment constitutes a ring buffer, that is independent of the
the other segments. The global header contains information global to
the entire buffer and is used for initialization, when a new processes attaches to the buffer. The parameters within the global
header are initializes by the processes creating the buffer and are supposed to be constant, for the lifetime of the buffer. An exception to this 
are the \texttt{writer attached} and \texttt{readers attached} fields, which will be covered later.
The \texttt{segment count} variable, indicates the amount of segments within the entire buffer. As illustrated
in figure \ref{fig:shm_architecture}, the number of segments is supposed to map 1:1 to the number 
of writing threads. Since each writing thread has their own segment in the buffer, no synchronization for access
to write pointers is required. The entries within the segments are fixed
size lines, the length of which in bytes is determined by the \texttt{line size} field in the global header. Each line corresponds
to one log message, the same way a line in a logfile would. 
\par 
Limiting buffer entries to a fixed maximum size has some obvious disadvantages. The primary concern
is inefficient use of memory, as the line size will have to be chosen, to accommodate the largest possible log message, since multi-line
messages are not intended in the design. If log messages vary greatly in length, this may cause a lot of unused memory
for shorter messages. The fixed size was chosen for a more convenient implementation of the \texttt{overwrite} option, will be covered later. The overwrite option is enabled by setting the corresponding field in the global header. 
Allowing for variable length entries, is still an important consideration for further development of the design.
\par
The \texttt{line count} variable in the global header 
determines the number of entries each segment of buffer has. Making all segments equal in size implicitly assumes, that the distribution of log messages written
is equal among the writing threads. However, variable size segments could be trivially added to the design, should a use case for that emerge. 
Finally, the global header contains a \texttt{reader count} field for the number of allowed readers, as well as boolean fields, that indicate the attachment of the writer and the readers.
Keeping the number of allowed readers fixed, limits the ability of dynamically adding new readers, but makes the implementation more convenient. The \texttt{readers attached} field is used for
reference counting, of how many reading processes are attached to the buffer. When a reader attaches to the buffer, it will iterate through the \texttt{readers attached} array in the global header, in search of the first entry, that is
set to 0. If an entry is found, the reader will set the value of the entry to 1 and the index of the entry becomes the readers \texttt{reader id}. Since writing to the attachment fields 
is implemented via atomic compare exchange operations, no race conditions exist for simultaneously attaching readers. Similarly, the writer uses the \texttt{writer attached} field when attaching to the buffer.
\par
Each segment of the buffer begins with its own header, that consist of a write index and an array of read indices, that is \texttt{reader count} elements long.
The indices correspond to a line in the segment and are used to keep track of the current position of the reader or writer in the buffer, as well
as to synchronize writer and readers. Both reader and writer indices are implemented as atomic variables. Hence, read and write operation 
on the indices can be executed, without interference, by a simultaneously executed operation. This eliminates the possibility of race-conditions, where a processes could potentially read a corrupted value. Using atomic indices, allows for lock free synchronization of reader and writer, which avoids the performance impact of having to
switch threads between waiting and execution states, that result from lock-based synchronization. However, atomic operations are also more costly, compared to regular load and store operations\footnote{The performance evaluation of atomic vs lock based synchronization is not part of this thesis, but could be a consideration for a potential further development of this architecture.}. 
The lines of the segment begin below the segment header. The segment buffer capacity in bytes, is given by \texttt{line size} $\times$ \texttt{line count}.  

\subsection{Implementation}
In the following, the implementation of the design in figure \ref{fig:shm_architecture} and relevant API functions are presented.
The entire API was implemented as a static library in C. The associated header and source files are \texttt{shm\_rbuf.h}, \texttt{io\_ipc.h} and \texttt{shm\_rbuf.c}, which can be found 
under \texttt{src/lib} in the Git repository for this thesis \cite{gitlab}. All API functions return numeric error codes, the meaning of which is documented in \texttt{io\_ipc.h}.  
\par
\begin{algorithm}[h!]
    \lstinputlisting[language=c, firstline=86, lastline=92]{listings/shm_ringbuf.h}
    \caption[Shared Memory Ringbuffer: Initialization and Cleanup]{Initialization and finalization / cleanup functions for the shared memory ring buffer.}
    \label{alg:shm:set_clean}
\end{algorithm}
Algorithm \ref{alg:shm:set_clean} displays the function signature for the initialization and finalizations functions.
\texttt{shmrbuf\_init} is used to create or attach to an existing shared memory buffer and has to be called, before read or write operations
on the buffer can be executed. \texttt{shmrbuf\_finalize} is the corresponding function to detach or destroy the shared memory buffer and should be called
upon ending the interaction with the buffer, for instance when exiting an application. \texttt{shmrbuf\_finalize} will only destroy the shared memory segment,
if no other process is attached. Otherwise the memory will simple be unmapped and the attachment field of the processes in the global header (as described in \ref{sec:rbuf_design}) is released. 
\texttt{shmrbuf\_init} and \texttt{shmrbuf\_finalize} can be called by both reader and writer 
processes, which is why a role has to be specified, as an additional parameter in the function call. The role determines, what type of structure the union pointer \texttt{shmrbuf\_arg\_t} is being cast to,
which contains further parameters used by the functions. 

\begin{algorithm}[h!]
    \lstinputlisting[language=c, firstline=27, lastline=36]{listings/shm_ringbuf.h}
    \caption[Shared Memory Ringbuffer: Writer Parameters]{Structure to store writer parameters.}
    \label{alg:shm:writer_arg}
\end{algorithm}

Algorithm \ref{alg:shm:writer_arg} displays the structure used by a writing processes, to store and pass related parameters to the API functions.
In the current implementation, creation of a buffer can only be performed by a writing process. The reason for this is, that parameters, such as the line
size or segment count, need to be tailored to the requirements of the writing process. The writer creates a new buffer, by parameterising a \texttt{shmrbuf\_writer\_arg\_t} structure with
the desired parameters for the buffer and calling \texttt{shmrbuf\_init}. In addition to the already discussed parameters, line size and line, segment and reader count, 
a key parameter is required. The key has to be a valid filepath within the file system and is used by System Vs shared memory API, to identify the shared memory segment\footnote{The System V shared memory API \cite{systemvshm} was used instead of the more modern POSIX API, since its support for memory allocation with huge pages was more convenient. Allocating the buffer with huge pages, should make read and write operations on a large buffer more efficient, since fewer entries are required in the translation lookaside buffer (TLB), making TLB misses less likely.}.  
If no shared memory segment referenced by the given path exists, a new segment of appropriate size will be allocated and mapped into the calling processes address space. Subsequently, its global header will 
be initialized with the parameters provided in the \texttt{shmrbuf\_writer\_arg\_t} structure.  

\begin{algorithm}[h!]
    \lstinputlisting[language=c, firstline=50, lastline=58]{listings/shm_ringbuf.h}
    \caption[Shared Memory Ringbuffer: Global Header]{Structure, to represent the global header of the shared memory ring buffer.}
    \label{alg:shm:global_hdr}
\end{algorithm}

Algorithm \ref{alg:shm:global_hdr} displays the structure for the global header of the shared memory buffer. When the header has been initialized,
a checksum over all constant header fields is calculated and saved to \texttt{checksum}. This is done, as an additional security check, for other processes attaching to the buffer. 
If the checksum is not correct, when attaching to an existing buffer, \texttt{shmrbuf\_init} will fail, to avoid writing to or reading from potentially corrupted memory. After initializing the header,
\texttt{shmrbuf\_init} sets the indices of all segment headers to zero. As an alternative to creating a new buffer,
a writer can also reattach to an existing buffer. When reattaching, the buffer parameters provided in \texttt{shmrbuf\_writer\_arg\_t} are ignored. To enable reattachment, the \texttt{SHMRBUF\_REATT} flags has to be specified in the flags field of the \texttt{shmrbuf\_writer\_arg\_t}, otherwise, the call   
to \texttt{shmrbuf\_init} will fail for an existing buffer. Reattachment is included in the API, so the writing application can be restarted (for instance for maintenance) without the need to
destroy the shared memory segment, if readers are attached. The overwrite feature is also specified via the flag field, by specifying the \texttt{SHMRBUF\_OVWR} flag.
Finally, \texttt{shmrbuf\_init} initializes an array of segment header structures in \texttt{shmrbuf\_writer\_arg\_t} and sets the \texttt{global\_hdr} pointer, to point to the start of the shared memory segment.

\begin{algorithm}[h!]
    \lstinputlisting[language=c, firstline=67, lastline=71]{listings/shm_ringbuf.h}
    \caption[Shared Memory Ringbuffer: Writer Segment Header]{Structure, to store writer information, for a segment of the shared memory ring buffer.}
    \label{alg:shm_seg_write}
\end{algorithm}
Algorithm \ref{alg:shm_seg_write} displays the structure used in the segment header array. The structure does not represent the actual segment header the way its layed out in memory, but holds convenience pointers for a segment, that are used for writing operations.   
   
\par
For readers, attaching to an existing buffer is also done with a call to \texttt{shmrbuf\_init}. This functions analogous the writer, with the exception, that the
shared memory segment, pointed to by the provided path, has to exist. 
\begin{algorithm}[h!]
    \lstinputlisting[language=c, firstline=39, lastline=46]{listings/shm_ringbuf.h}
    \caption[Shared Memory Ringbuffer: Reader Parameters]{Structure to store reader parameters.}
    \label{alg:shm:reader_arg}
\end{algorithm}
Algorithm \ref{alg:shm:reader_arg} displays the structure used for storing reader parameters. If the attachment to the shared memory segment
and the checksum were successful, the reader will obtain a unique \texttt{reader id}, as described in section \ref{sec:rbuf_design}. If all reader slots are already occupied, the \texttt{shmrbuf\_init} call fails.
Subsequently, \texttt{shmrbuf\_init} will initialize the pointers to the global header and the array of segment headers structures in the provided \texttt{shmrbuf\_reader\_arg\_t} structure.  
Readers have their own structure for storing segment header information, which is displayed in Algorithm \ref{alg:shm:seg_read}.  
\begin{algorithm}[h!]
    \lstinputlisting[language=c, firstline=60, lastline=65]{listings/shm_ringbuf.h}
    \caption[Shared Memory Ringbuffer: Reader Segment Header]{Structure, to store reader information, for a segment of the shared memory ring buffer.}
    \label{alg:shm:seg_read}
\end{algorithm}

The structure includes a pointer to the index of the reader within the segment header. Which read index of a segment header is assigned to a reader, is determined 
by their \texttt{reader id}, that corresponds to an index. Additionally, the structure contains a mutex lock, that is used to synchronize access to the segment, when reding with multiple threads.
After a successful call to \texttt{shmrbuf\_init}, both reader an writer can start using their respective APIs for the buffer.

\subsection{Write API}

\begin{algorithm}[h!]
    \lstinputlisting[language=c, firstline=94, lastline=103]{listings/shm_ringbuf.h}
    \caption[Shared Memory Ringbuffer: Write API]{Write API for the shared memory ring buffer.}
    \label{alg:shm:write_api}
\end{algorithm}

Algorithm \ref{alg:shm:write_api} displays the write API for the shared memory ring buffer. \texttt{shmrbuf\_write}
writes a single log message to a specified segment within the buffer. The segment being written to has to be reference with an id, which is simply its index within $\{0..n\}$, where
n is the number of segments. The \texttt{src} argument is a pointer to the message, that should be written to the buffer and \texttt{wsize} specifies the length of the message.
If \texttt{wsize} exceeds the line size specified in the global header of the buffer, the write operation will fail with a size error. When issuing a call to \texttt{shmrbuf\_write} without overwrite, the 
the function will atomically load all reader indices in the segment header and check their distance to the write index. If no reader is directly ahead of the write index, the line will be written
to the buffer and the writer index will be atomically updated to its new position, otherwise, the function call returns with a size error. If the overwrite operation is specified in the global header, synchronization between writer and reader is
effectively disabled and the write operation will conclude, regardless of the readers positions. This incurs the risk, that a reader and writer simultaneously access 
the same line, which may potentially corrupt the data being read. An advantage of overwrite is, that the writer avoids costly atomic operations and also cant be blocked 
by a slow or stale reader. The performance of the overwrite feature will be evaluated in section \ref{sec:evaluation}.
The write API provides a seconds write function in \texttt{shmrbuf\_writev}. Instead of writing a single line, the function receives an  
 array of \texttt{iovec} structures. The \texttt{iovec} structure, defined in \texttt{struct\_iovec.h} within the standard C library, is a generic container for vectored
\ac{IO} operations. It contains a void pointer to a data field and a variable, to specify the size of the data field. 
\texttt{shmrbuf\_writev} works analogous to \texttt{shmrbuf\_write}, with the only difference, that \texttt{vsize} lines referenced by the corresponding \texttt{iovec}
structure in iovecs will be written to the buffer. Performance inspection of \texttt{shmrbuf\_write} with the benchmarking tool
perf \cite{perf} revealed, that about 75\% of the execution time is being spent for executing the atomic operations. Hence, 
using a vectored write operation should improve performance, as the synchronization only has to be applied once for multiple log messages.
For the sake of performance, neither write function implements synchronization for the write index. Calling a write operation
on the same segment from two different threads, is therefore not thread-safe.

\subsection{Read API}

Algorithm \ref{alg:shm:read_api} presents the functions of the reader API.
\begin{algorithm}[h!]
    \lstinputlisting[language=c, firstline=105, lastline=133]{listings/shm_ringbuf.h}
    \caption[Shared Memory Ringbuffer: Read API]{Read API for the shared memory ring buffer.}
    \label{alg:shm:read_api}
\end{algorithm}

\texttt{shmrbuf\_read} is the corresponding read operation to \texttt{shmrbuf\_write}. It reads a single from
the specified segment into the provided buffer. If the provided buffer size is exceeded by the
size of the line, the read call fails. Hence, the buffer should always be able to contain the maximum line size, specified 
in the global header. Upon a call to \texttt{shmrbuf\_read}, the function atomically loads the write index for the segment. 
If the position of the write index differs from that of the readers read index, the line pointed to by 
the reader index will be copied to the external buffer and the read index will be atomically updated to its new position.
On success, \texttt{shmrbuf\_read} returns the (maximum) line size or zero, if the buffer is empty \footnote{In its current implementation, the buffer allows any type of binary data to be copied to a line. This is a problem for determining the actual length of a string
in the line buffer, since the terminating zero byte could still be part of the payload. Therefore, the maximum line length is always returned. Restricting the buffer to
non zero characters, or moving to a variable line length design, would enable the function to determine the length of the string contained in the line buffer, making the return value more meaningful.}.     
Analogous to the write API, a vectored read function exists in \texttt{shmrbuf\_readv}, which should incur the same performance benefits.
Instead of reading a single line, \texttt{vsize} lines are copied to the buffers specified by the corresponding \texttt{iovec} structures in \texttt{iovecs}.
Unlike the write API, all read operations called on the same segment are thread-safe. 
At the beginning of each read operation, the mutex lock in the corresponding \texttt{shmrbuf\_seg\_rhdr\_t} structure is claimed and released, after the read index has been updated.
For performance reasons, frequent locking of a segment by different reading threads should be avoided and threads should ideally be assigned segments that they access exclusively.
\par
The read API contains two further functions. \texttt{shmrbuf\_read\_rng} is a convenience wrapper around \texttt{shmrbuf\_read}. It its useful for the case, where the reading
application uses fewer reading threads, than there are segments in the buffer. \texttt{shmrbuf\_read\_rng} allows the specification of a segment range, through the \texttt{upper}
and \texttt{lower} parameters. When calling \texttt{shmrbuf\_read\_rng}, the function will iterate over all segments in the range, in a round-robin fashion and perform a read call, until the first non empty segment is found or one cycle is completed.
The iteration index is persistent across function calls, ensuring that subsequent calls will continue the round-robin at the same position. This allows
starvation-free reading from the specified segment range. The boolean pointer \texttt{wsteal} allows for the option of workload stealing.
If \texttt{wsteal} is non null and all segments within the specified range are empty, the function will additionally iterate over all
segments outside of the specified range, until a line has been read successfully or all segments have been checked once. If a line was read from 
outside the specified range, \texttt{wsteal} will be set to 1. The intention of the workload stealing feature, is to mitigate scenarios,
where writing operation are not equally distributed across segments. If a reader, for instance, assigns two reading threads to one segments each,
but only one of the segments is being written to, then one reading thread would be idle, while the other has to handle all reading operations.
Workload stealing mitigates this, by having threads read across their assigned boundaries, if they would otherwise be idle.
\texttt{shmrbuf\_readv\_rng} is the last function in the read API and corresponds to \texttt{shmrbuf\_read\_rng}, with the only difference,
that multiple lines can be read from each segment, up to the number specified by \texttt{vsize}. The \texttt{wsteal} parameter is
also an intege,r instead of a boolean pointer and will be set to the number of lines read from outside the specified range, if \texttt{wsteal} is non null.

\section{Proof-of-Concept IPS}

The following section presents the design and implementation of the proof of concept \ac{IPS}, for testing the \ac{IPC} architecture covered in the past sections.
In order to be comparable, the \ac{IPS} will be closely modelled after Fail2ban. The goal is however not, to reimplement Fail2bans full set of feature, as that would go beyond
the scope of this thesis. Instead, a minimal set of features will be supported, the allows for the replication of the experiment covered in section \ref{sec:fail2ban}. 
The requirements for the \ac{PoC} are as follows:
\begin{itemize}
    \item Ability to monitor an application log, via traditional logfile parsing and the new \ac{IPC} solution.
    \item Ability to parse log messages with a custom regular expression.
    \item Ability to create filter rules for a clients IP addresses, for both \ac{IPv4} \& \ac{IPv6}.
    \item Support for a ban limit, i.e. a custom number of matches per client, before a ban is executed.
    \item Support for a ban time, i.e. a custom duration, for the ban of a clients IP address. 
\end{itemize} 

Figure \ref{fig:ips_architecture} illustrates the proposed design for the \ac{PoC}, which will from hereon be referred to as Simplefail2ban. The source file, 
for the implementation of Simplefail2ban is \texttt{simplefailban.c} in the \texttt{src/programs} directory of the thesis Git repository \cite{gitlab}.


\begin{figure}[p]
    \includegraphics[width=\textwidth]{images/ips_architecture.png}
    \caption[Simplefail2ban Architecture]{Activity diagram for the proof-of-concept IPS implementation. A variable number of ``banning threads'' receive log messages from a host and
    parse them, with a predefined regular expressions. For messages that match the expression, the clients IP address is extracted from the log message and added to a hash table, that keeps count of
    the number of matches per address. If the count reaches the configured limit, the address is added to the list of banned addresses, with a current timestamp and inserted into the eBPF map. One ``unbanning
   thread'' routinely iterates through the banned list and checks, if a clients ban time hast elapsed. Clients with an elapsed ban time are removed from the eBPF map, banned list and hash table.}
   \label{fig:ips_architecture}
\end{figure}

The functionality of the application will be separated into two classes of threads. A variable number of ``banning'' threads
and a single ``unbanning'' thread. The banning threads are tasked with monitoring the application log. They routinely call a read
function, to check, if new messages have been added to the log. The read function varies, depending on the type of \ac{IPC} architecture
being used. For multi-threaded use, the read function has to be thread-safe and should ideally be lock free, to allow for optimal performance.
When using the shared memory ring buffer as the source for log messages, \texttt{shmrbuf\_readv\_rng} is used, where the buffer segments
are equally distributed among the banning threads. For traditional logfile parsing, a custom read function was implemented, on the
basis of the asynchronous \ac{IO} library liburing, introduced in \ref{sec:io_uring}.

\begin{algorithm}[h!]
    \lstinputlisting[language=c, firstline=27, lastline=35]{listings/uring_getline.h}
    \caption[Asynchronous Getline Function]{Function signatures for the io\_uring based getline functions.}
    \label{alg:uring_getline}
\end{algorithm}

\ref{alg:uring_getline} displays the function signatures, for the liburing based read functions. \texttt{uring\_getline} was modelled after the getline function
from \texttt{stdio.h} within the C standard library \cite{getline}.  The source file, 
for the implementation is \texttt{uring\_getline.c} in the \texttt{src/lib} directory of the thesis Git repository \cite{gitlab}. The function is passed a pointer to a \texttt{file\_io\_t} structure,   
which contains a file descriptor for the designated file, as well two buffers, for the data being read. When the function is called, both buffers are checked for lines remaining from an earlier function call.
If both buffers are empty, a liburing based read operation
is initiated. The read data is then tokenized into lines, via a linear search for newline characters. If the read operation fills the entire buffer, another read
operation is scheduled for the second buffer. The return of the read operation is not awaited and will only be checked on a subsequent function call. The idea is,
that, in anticipation of subsequent function calls, data is being asynchronously read ahead, so that for most function calls, the function can resort to the buffer, instead
of reading from the actual file. This should hopefully be more performant than regular blocking calls to \texttt{read}. The file offset for the read operation is tracked in
the \texttt{file\_io\_t} structure and updated after each read. \texttt{uring\_getlines} works analogous to \texttt{uring\_getline}, with the difference, that up to \texttt{vsize}
lines can be read in a single function call. The lines and their size in byte are copied to the corresponding \texttt{iovec} structure in \texttt{iovecs}.
\texttt{uring\_getline} and \texttt{uring\_getlines} are not build for thread-safety, hence, the logfile based version of Simplefail2ban only supports single threaded monitoring\footnote{It its unclear, wether multi-threaded file reading would provide a performance benefit, since the read operations would have to be serialized by the operating system. Hence, the decision to not support multi-threading for the logfile variant of Simplefailban.}.
\par
If a banning thread has successfully read one or multiple log messages, the messages are then matched against a regular expressions, which can be defined at the start of the application.
For \ac{Regex} matching, the regular expressions engine Hyperscan, introduced in section \ref{sec:hyperscan}, is used. Hyperscan is used in multi-pattern matching mode, scanning
for both the provided \ac{Regex}, as well as regular expressions for \ac{IPv4} and \ac{IPv6} addresses, to extract the clients \ac{IP} address from the log message. If the message was successfully matched against
the provided \ac{Regex}, the clients \ac{IP} address, identified by the \ac{IP}-\ac{Regex}, will be translated to its binary form, for further processing. If the scan does not find a match, 
the processing moves on to the next log message. Alternatively, Simplefail2ban can also be configured to directly translate the entire log message to an \ac{IP} address, without regular expression matching.
This is not explicitly referenced in \ref{fig:ips_architecture} and was only implemented as reference, to measure the impact of \ac{Regex}-matching on the overall performance of the application.
\par
Once a clients binary \ac{IP} address has been determined (either through successful matching or direct translation), a hash table lookup for the address is performed.
The purpose of this, is to determine, if a client has reached the ban limit, which is configured at the start of the application. 
The hash table stores key value pairs, where the key can be a \ac{IPv4} or \ac{IPv6} address in binary form and the value is a counter,
indicating the number of times the key has been queried. A custom hash table was implemented, as a static library, the sources files for which are \texttt{ip\_hashtable.h}
and \texttt{ip\_hashtable.c} in the \texttt{src/lib} directory of the thesis Git repository \cite{gitlab}. 

\begin{algorithm}[h!]
    \lstinputlisting[language=c, firstline=29, lastline=38]{listings/ip_hashtable.h}
    \caption[IP Hash Table]{Structure for storing a single entry in the IP hash table. The \texttt{key} pointer 
    points to the binary address, the size of which depends on wether the domain value is \texttt{AF\_INET} or \texttt{AF\_INET6}.}
    \label{alg:ip_hashtable}
\end{algorithm}

\ref{alg:ip_hashtable} displays the structure used for storing a single entry in the hash table. The base
table consist of an array of bins, with a default size of 6000011\footnote{The array size was purposefully chosen as a prime number, to allow for a more equal spread among the bins.}.
When an \ac{IP} address is inserted into the table, an index within the array is determined by calculated the value of a hash function modulo 
the size of the array. The address and its associated counter value are then stored in the bin at the determined index, if it is empty. 
The hash function used for the implementation is spookyc, which is a C implementation of
the spooky hash function by Bob Jenkins \cite{spookyc}. If the bin at the determined is not empty, a collision occurs.
Collisions are handled via a linked list, where the bin in the base array constitutes the first node of the list.
\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{llll}
        \toprule
        \textbf{Number of Insertions} & \textbf{Collisions IPv4 [\%]} & \textbf{Collisions IPv6 [\%]} & \textbf{Collisions IPv4 \& IPv6 [\%]}\\ \midrule 
        65534 & 5.33 & 5.29 & 5.28 \\ \midrule
        131068 & 10.17 & 10.17 & 10.15 \\ \midrule
        600011 & 36.81 & 36.77 & 36.8 \\
        \bottomrule
    \end{tabular}
    \caption[Hash Collisions]{Percentage of hash collisions by key type for different numbers of insertions. The results were obtained
    with \texttt{hashfunc\_benchmark.c} in the \texttt{src/utilities} directory of the thesis Git repository \cite{gitlab}.}
    \label{tab:hash_col}
\end{table}

Table \ref{tab:hash_col} presents experimental results for the collisions performance of the hash function. 
There appear to be no significant differences in collision behaviors between \ac{IPv4} and \ac{IPv6} addresses.
For 6000011 insertions (size of the base table), a little more than a third of all insertions resulted in a collision. 
The hash table is global to the application, since different banning threads may handle log messages for the same
client and need to synchronize their match count. To ensure thread-safe operations on the hash table, locking via mutexes is used.
The granularity of the locking is at bin level, hence, parallel operations on the table are possible ,if different bins are accessed.
\par
If the hash table lookup returns a count that is equal to the ban limit, the banning thread facilitates a ban
action, analogous to Fail2ban. To implement \ac{IP} address filtering, the \ac{eBPF} program developed by Florian Mikolajczak,
 as described in \ref{sec:fail2ban}, was used. The program can be loaded onto an network interface and is executed event based on incoming packets\footnote{For a more through and detailed explanation, see \cite{mikolajczak2022}}.
To determine which packets should be dropped, the program uses maps, containing \ac{IP} addresses, which are pinned to the \ac{eBPF} file system.
At the start of Simplefail2ban, the \ac{eBPF} program will be loaded onto a configured interface. The function used for loading and unloading
the \ac{eBPF} program and corresponding maps are slightly adapted from the implementation by  Florian Mikolajczak and can be found in \texttt{ebpf\_helpers.h}
and \texttt{ebpf\_helpers.c} in the \texttt{src/lib} directory of the thesis Git repository \cite{gitlab}. The ban action, executed by the banning thread,
consists of adding a clients binary \ac{IP} address to the corresponding \ac{eBPF} map. There are two distinct maps being used for \ac{IPv4} and \ac{IPv6} addresses.
Once the address has been added to the map, the \ac{eBPF} program will drop all incoming packets from that address, for as long as the address is contained in the map.
Additionally, the banning thread adds the \ac{IP} address to a linked list (referred to as banned list in \ref{fig:ips_architecture}) together with a 
current timestamp. The linked list is used to store client, that are currently banned. Source files for the linked list implementation are \texttt{ip\_llist.h}
and \texttt{ip\_llist.c} and can be found in the \texttt{src/lib} directory of the thesis Git repository \cite{gitlab}. The unbanning thread will routinely iterate over the list 
and check the amount of time that has passed since the timestamp, for each entry. If the configured ban time has elapsed, the unbanning thread will unban the client.
To unban a client, the unbanning thread removes its entry in the linked list, as well as the entry in the \ac{eBPF} map. Finally, the address entry in the
hash table is removed. This resets the matching count, so a client can be banned, if the ban limit is reached again.
\par
The functions, implementing the described routines for banning and unbanning thread are \texttt{ban\_thread\_routine} 
and \texttt{unban\_thread\_routine} within \texttt{simplefailban.c}.   

\section{Test Application} \label{sec:test_server}

To evaluate the \ac{IPC} architecture in conjunction with Simplefail2ban, a test application is needed, which utilizes the \ac{IPC} API to transmit log
messages. For this purpose, a test server was developed, the source file for which is \texttt{udp\_server.c} in the \texttt{src/programs} directory of the thesis Git repository \cite{gitlab}. 
Alternatively, the \ac{IPC} architecture could have been integrated into a real application, like the BIND server, used in the previous Fail2ban measurements by Florian Mikolajczak. While this would have provided a more 
realistic basis for the evaluation, the associated implementation effort was deemed beyond the scope of this thesis. 
The test server hence serves as a stand in, for a real \ac{UDP} based application such as BIND. It listens on a configurable port,
and replies to incoming packets with a single, one byte payload UDP packet. Based on the first payload byte of the received request,
the server decides, wether to write a log string for the requesting client\footnote{The byte to trigger logging can be determined with the \texttt{INVALID\_PAYLOAD} macro in \texttt{udp\_server.c}.}.
The log string contains the current date, time and \ac{IP} address of the client, as well as a descriptive message and is written to either a logfile or the shared memory ring buffer.  
Alternatively, the test server can be configured, to only log a clients \ac{IP} address. The test server uses multiple threads to listen for incoming packets 
and is designed to handle a large amount of requests and logging operation per second, in order to not be a bottleneck for the evaluation of Simplefail2ban. 
Perf \cite{perf} evaluation of the test server revealed, that about 20\% of the execution time on the application side was spent translating \ac{IP} addresses to string form.  
To facilitates fast creation of log strings, a custom function was written for the transformation of a binary \ac{IP} address to string form. 
\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{lll}
        \toprule
        \textbf{Function} & \textbf{Execution Time IPv4 [Seconds]} & \textbf{Execution Time IPv6 [Seconds]} \\ \midrule 
        \texttt{inet\_ntop} & 1.29 & 3.98 \\ \midrule
        \texttt{ip\_to\_str} & 0.21 & 0.53 \\ 
        \bottomrule
    \end{tabular}
    \caption[IP String Conversion]{Performance evaluation for binary IP address to string conversion. The evaluated functions are
    \texttt{inet\_ntop} from \texttt{arpa/inet.h} in the C standard library and a custom function \texttt{ip\_to\_str}. The corresponding source 
    files are \texttt{ip\_to\_str.h}
    and \texttt{ip\_to\_str.c} in the \texttt{src/lib} directory and the evaluation was conducted with
    \texttt{ip\_string\_benchmark.c} in the \texttt{src/utilities} directory of the thesis Git repository \cite{gitlab}}
    \label{tab:ip_str}
\end{table}

Table \ref{tab:ip_str} summaries the evaluation results for the custom function \texttt{ip\_to\_str} and the standard library function \texttt{inet\_ntop}. 
\texttt{ip\_to\_str} is, on average, about 6 times faster for translating \ac{IPv4} addresses and about 7 times faster for \ac{IPv6}. Since the test server
can write up to a million log messages per second, overall performance should be improved by the use of \texttt{ip\_to\_str}\footnote{This could be further evaluated, by the comparing overall difference of the server for both functions, but I unfortunately did not have the time to implement this test.}. 

\section{Other Applications} \label{sec:other_app}

Two other applications where developed as part of this thesis. To test the multi reader capability of the proposed
shared memory architecture, an application serving as an additional reader was implemented. The application models a 
log aggregator such as Logstash (discussed in \ref{sec:ipc_types}) and writes the log messages contained in the buffer to a configurable 
logfile. The source file for this application is \texttt{simplelogstash.c} in the \texttt{src/utilities} directory of the thesis Git repository \cite{gitlab}.
The second application is a utility for inspection and debugging the shared memory ring buffer. It allows the inspection of the global header, as well
as the display of load statistics for the individual segments and the entire buffer. The source file is \texttt{poll\_rbuf.c} in the \texttt{src/utilities} directory of the thesis Git repository \cite{gitlab}. 

